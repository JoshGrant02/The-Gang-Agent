{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#import useful libraries\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from scipy import interpolate\n",
    "from scipy.stats import truncnorm\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from pydantic import BaseModel\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List\n",
    "\n",
    "from game import TheGang\n",
    "from models import HandFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, hidden_layer_count = 1):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(input_size, hidden_layer_size))\n",
    "        nn.init.normal_(self.hidden_layers[0].weight, mean = 0, std = 0.01)\n",
    "        nn.init.normal_(self.hidden_layers[0].bias, mean = 0, std = 0.01)\n",
    "        for _ in range(hidden_layer_count-1):\n",
    "            layer = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "            self.hidden_layers.append(layer)\n",
    "            nn.init.normal_(layer.weight, mean = 0, std = 0.01)\n",
    "            nn.init.normal_(layer.bias, mean = 0, std = 0.01)\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, 4)    #the output layer with probabiliy for each action\n",
    "        self.output_layer.bias = nn.Parameter(torch.tensor([0.0, 0.0, 0.0, 0.0])) \n",
    "\n",
    "    def forward(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        '''A function to do the forward pass\n",
    "            Takes:\n",
    "                s -- the state representation\n",
    "            Returns:\n",
    "                a tensor of probabilities\n",
    "        '''         \n",
    "        s = s.to(next(self.parameters()).device)\n",
    "        for layer in self.hidden_layers:\n",
    "             s = torch.relu(layer(s))    #pass through the hidden layers\n",
    "        s = self.output_layer(s)\n",
    "        action_probs = torch.softmax(s, dim=1)    #use softmax to get action probabilities\n",
    "        return action_probs.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentConfig(BaseModel):\n",
    "    hidden_layer_size: int = 16\n",
    "    hidden_layer_count: int = 16\n",
    "    learning_rate: float = 0.0005\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, config: AgentConfig):\n",
    "        self.config = config\n",
    "        self.pi = PolicyNetwork(17, config.hidden_layer_size, config.hidden_layer_count).to(device)\n",
    "        self.optimizer = optim.Adam(self.pi.parameters(), lr=self.config.learning_rate)\n",
    "\n",
    "    def generate_actions(self, state_array: List[List[int]]):\n",
    "        action_probs = self.pi(torch.tensor(state_array, dtype=torch.float32)) # Create tensor and feed state through model\n",
    "        sampled_actions = torch.multinomial(action_probs, 1).squeeze(dim=1)\n",
    "        action_space = torch.tensor([1, 2, 3, 4])\n",
    "        final_actions = action_space[sampled_actions]\n",
    "        return final_actions\n",
    "\n",
    "    def generate_action_probs(self, a: List[int], state_array: List[List[int]]):\n",
    "        all_action_probs = self.pi(torch.tensor(state_array, dtype=torch.float32)) # Create tensor and feed state through model\n",
    "        a_tensor = torch.tensor(a, dtype=torch.long)\n",
    "        a_tensor = a_tensor - 1\n",
    "        performed_action_probs = all_action_probs.gather(1, a_tensor.unsqueeze(1)).squeeze(1)\n",
    "        log_probs = torch.log(performed_action_probs)\n",
    "        return log_probs\n",
    "\n",
    "    def checkpoint(self, model_name, epoch_count: int):\n",
    "        directory = f'checkpoints/{model_name}'\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        torch.save(self.pi.state_dict(), f'{directory}/epi_{epoch_count}.pth')\n",
    "        \n",
    "    def load_checkpoint(self, model_name: str, epoch_count: int):\n",
    "        self.pi.load_state_dict(torch.load(f'checkpoints/{model_name}/epi_{epoch_count}.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig(BaseModel):\n",
    "    discount_factor: float = 0.5\n",
    "    batch_size: int = 100\n",
    "    checkpoint_freq: int = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "    agent: Agent,\n",
    "    env: TheGang,\n",
    "    training_config: TrainingConfig,\n",
    "    num_epochs: int,\n",
    "    starting_epoch: int,\n",
    "    model_name: str\n",
    "):\n",
    "    average_epoch_rewards = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_batch_rewards = []\n",
    "        batch = []\n",
    "        # Repead episodes until epoch batch size is reached\n",
    "        for epi in range(training_config.batch_size):\n",
    "            env.reset()\n",
    "\n",
    "            episode_rewards = []\n",
    "            episode_states = []\n",
    "            episode_actions = []\n",
    "            state = {'state':env.generate_state_array([0,0,0,0]),'reward':[0,0,0,0],'done':False}\n",
    "\n",
    "            # Loop turns in episode\n",
    "            while not state['done']:\n",
    "                episode_states.append(state['state'])\n",
    "                a = agent.generate_actions(state['state']).tolist()\n",
    "                episode_actions.append(a)\n",
    "                state = env.step(a)\n",
    "                episode_rewards.append(state['reward'])\n",
    "\n",
    "            # Track fire rate and terminal rewards for non-fired episodes\n",
    "            epoch_batch_rewards.extend(episode_rewards)\n",
    "\n",
    "            # Calcualte causal reward\n",
    "            causal_returns = []\n",
    "            rolling_causal_return = np.zeros(len(episode_rewards[0]))\n",
    "            # Calculate causal returns in reverse order\n",
    "            for rewards in episode_rewards[::-1]:\n",
    "                causal_return = rewards + training_config.discount_factor * rolling_causal_return\n",
    "                rolling_causal_return = causal_return\n",
    "                causal_returns.append(causal_return)\n",
    "            # Reverse the causal returns to get the correct order\n",
    "            causal_returns = causal_returns[::-1]\n",
    "\n",
    "            for state, action, reward, causal_return in zip(episode_states, episode_actions, episode_rewards, causal_returns):\n",
    "                batch.append({'s_t': state, 'a_t': action, 'r_t': reward, 'cr_t': causal_return})\n",
    "\n",
    "        # Calculate epoch performance\n",
    "        average_epoch_rewards.append(np.mean(epoch_batch_rewards))\n",
    "        \n",
    "        # Checkpoint if applicable\n",
    "        if epoch % training_config.checkpoint_freq == 0:\n",
    "            agent.checkpoint(model_name, starting_epoch + num_epochs)\n",
    "            with open(f'checkpoints/{model_name}/epoch_rewards.csv', 'w', newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(epoch_batch_rewards)\n",
    "\n",
    "        log_probs = []\n",
    "        batch_causal_returns = []\n",
    "        for step in batch:    # Loop over batch\n",
    "            states = step['s_t']\n",
    "            actions = step['a_t']\n",
    "            action_probs = agent.generate_action_probs(actions, states)    # Compute action probability from policy\n",
    "            log_probs.append(action_probs.squeeze())    # Record the log probability of the chosen action\n",
    "            batch_causal_returns.append(step['cr_t'])\n",
    "\n",
    "        # Perform baselining\n",
    "        baseline = np.mean(batch_causal_returns)\n",
    "        baselined_causal_returns = batch_causal_returns - baseline\n",
    "\n",
    "        baselined_causal_returns = torch.tensor(baselined_causal_returns)\n",
    "        log_probs = torch.stack(log_probs)    #reshape to compute gradient over the whole episode #Shape: (batch_size, 4)\n",
    "\n",
    "        objective = -torch.sum(log_probs * baselined_causal_returns)/len(batch) # Batch, and do \"-\" to convert \"loss\" to \"gain\"\n",
    "\n",
    "        agent.optimizer.zero_grad()    #zero gradients from the previous step\n",
    "        objective.backward()    #compute gradients\n",
    "        agent.optimizer.step()\n",
    "\n",
    "    agent.checkpoint(model_name, starting_epoch + num_epochs)\n",
    "    with open(f'checkpoints/{model_name}/epoch_rewards.csv', 'w', newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(epoch_batch_rewards)\n",
    "\n",
    "    return epoch_batch_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(agent.pi.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/40000 [00:15<14:10:16,  1.28s/it]"
     ]
    }
   ],
   "source": [
    "agent_config = AgentConfig()\n",
    "training_config = TrainingConfig()\n",
    "agent = Agent(agent_config)\n",
    "env = TheGang()\n",
    "fire_rate, average_terminal_states = train_agent(agent, env, training_config, 40000, 0, 'Baseline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
