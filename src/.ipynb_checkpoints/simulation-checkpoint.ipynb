{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#import useful libraries\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from scipy import interpolate\n",
    "from scipy.stats import truncnorm\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from pydantic import BaseModel\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List\n",
    "import time\n",
    "\n",
    "from game import TheGang\n",
    "from models import HandFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@\n",
    "#Simple profiler class\n",
    "#@@@@@@@@@@@@@@@@@@@@@\n",
    "class Profiler:\n",
    "    def __init__(self):\n",
    "        self.checkpoint_times: Dict[str, List[float]] = {}\n",
    "        self.checkpoint_start_times: Dict[str, float] = {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.checkpoint_times: Dict[str, List[float]] = {}\n",
    "        self.checkpoint_start_times: Dict[str, float] = {}\n",
    "\n",
    "    def start(self, checkpoint: str):\n",
    "        start_time = time.time()\n",
    "        self.checkpoint_start_times[checkpoint] = start_time\n",
    "\n",
    "    def stop(self, checkpoint: str):\n",
    "        stop_time = time.time()\n",
    "        start_time = self.checkpoint_start_times.get(checkpoint, 0)\n",
    "        if start_time == 0:\n",
    "            raise Exception(\"No start time for the current checkpoint\")\n",
    "        duration = stop_time - start_time\n",
    "        existing_checkpoint = self.checkpoint_times.get(checkpoint, None)\n",
    "        if existing_checkpoint:\n",
    "            existing_checkpoint.append(duration)\n",
    "        else:\n",
    "            self.checkpoint_times[checkpoint] = [duration]\n",
    "\n",
    "    def report(self):\n",
    "        for checkpoint, values in self.checkpoint_times.items():\n",
    "            iters = len(values)\n",
    "            max_time = max(values)\n",
    "            min_time = min(values)\n",
    "            average_time = np.mean(values)\n",
    "            range = max(max_time - average_time, average_time - min_time)\n",
    "            print(\"%-20s | %.4f s Â± %.4f s per iteration | n_iters = %d\" % (checkpoint[:20], average_time, range, iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, hidden_layer_count = 1):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(input_size, hidden_layer_size))\n",
    "        nn.init.normal_(self.hidden_layers[0].weight, mean = 0, std = 0.01)\n",
    "        nn.init.normal_(self.hidden_layers[0].bias, mean = 0, std = 0.01)\n",
    "        for _ in range(hidden_layer_count-1):\n",
    "            layer = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "            self.hidden_layers.append(layer)\n",
    "            nn.init.normal_(layer.weight, mean = 0, std = 0.01)\n",
    "            nn.init.normal_(layer.bias, mean = 0, std = 0.01)\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, 4)    #the output layer with probabiliy for each action\n",
    "        self.output_layer.bias = nn.Parameter(torch.tensor([0.0, 0.0, 0.0, 0.0])) \n",
    "\n",
    "    def forward(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        '''A function to do the forward pass\n",
    "            Takes:\n",
    "                s -- the state representation\n",
    "            Returns:\n",
    "                a tensor of probabilities\n",
    "        '''         \n",
    "        s = s.to(next(self.parameters()).device)\n",
    "        for layer in self.hidden_layers:\n",
    "             s = torch.relu(layer(s))    #pass through the hidden layers\n",
    "        s = self.output_layer(s)\n",
    "        action_probs = torch.softmax(s, dim=1)    #use softmax to get action probabilities\n",
    "        return action_probs.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentConfig(BaseModel):\n",
    "    hidden_layer_size: int = 32\n",
    "    hidden_layer_count: int = 8\n",
    "    learning_rate: float = 0.0005\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, config: AgentConfig):\n",
    "        self.config = config\n",
    "        self.pi = PolicyNetwork(17, config.hidden_layer_size, config.hidden_layer_count).to(device)\n",
    "        self.optimizer = optim.Adam(self.pi.parameters(), lr=self.config.learning_rate)\n",
    "\n",
    "    def generate_actions(self, state_array: List[List[int]]):\n",
    "        action_probs = self.pi(torch.tensor(state_array, dtype=torch.float32)) # Create tensor and feed state through model\n",
    "        sampled_actions = torch.multinomial(action_probs, 1).squeeze(dim=1)\n",
    "        action_space = torch.tensor([1, 2, 3, 4])\n",
    "        final_actions = action_space[sampled_actions]\n",
    "        return final_actions\n",
    "\n",
    "    def generate_action_probs(self, a: List[int], state_array: List[List[int]]):\n",
    "        all_action_probs = self.pi(torch.tensor(state_array, dtype=torch.float32)) # Create tensor and feed state through model\n",
    "        a_tensor = torch.tensor(a, dtype=torch.long)\n",
    "        a_tensor = a_tensor - 1\n",
    "        performed_action_probs = all_action_probs.gather(1, a_tensor.unsqueeze(1)).squeeze(1)\n",
    "        log_probs = torch.log(performed_action_probs)\n",
    "        return log_probs\n",
    "\n",
    "    def checkpoint(self, model_name, epoch_count: int):\n",
    "        directory = f'checkpoints/{model_name}'\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        torch.save(self.pi.state_dict(), f'{directory}/epi_{epoch_count}.pth')\n",
    "        \n",
    "    def load_checkpoint(self, model_name: str, epoch_count: int):\n",
    "        self.pi.load_state_dict(torch.load(f'checkpoints/{model_name}/epi_{epoch_count}.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig(BaseModel):\n",
    "    discount_factor: float = 0.5\n",
    "    batch_size: int = 100\n",
    "    checkpoint_freq: int = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "    agent: Agent,\n",
    "    env: TheGang,\n",
    "    training_config: TrainingConfig,\n",
    "    num_epochs: int,\n",
    "    starting_epoch: int,\n",
    "    model_name: str,\n",
    "    profiler: Profiler = None\n",
    "):\n",
    "    average_epoch_rewards = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if profiler:\n",
    "            profiler.start(\"epoch\")\n",
    "\n",
    "        epoch_batch_rewards = []\n",
    "        batch = []\n",
    "        # Repead episodes until epoch batch size is reached\n",
    "        for epi in range(training_config.batch_size):\n",
    "            if profiler:\n",
    "                profiler.start(\"episode\")\n",
    "    \n",
    "            env.reset()\n",
    "\n",
    "            episode_rewards = []\n",
    "            episode_states = []\n",
    "            episode_actions = []\n",
    "            state = {'state':env.generate_state_array([0,0,0,0]),'reward':[0,0,0,0],'done':False}\n",
    "\n",
    "            # Loop turns in episode\n",
    "            while not state['done']:\n",
    "                episode_states.append(state['state'])\n",
    "                \n",
    "                if profiler:\n",
    "                    profiler.start(\"action generation\")\n",
    "                \n",
    "                a = agent.generate_actions(state['state']).tolist()\n",
    "                \n",
    "                if profiler:\n",
    "                    profiler.stop(\"action generation\")\n",
    "                    profiler.start(\"environment step\")\n",
    "                \n",
    "                episode_actions.append(a)\n",
    "                state = env.step(a)\n",
    "                episode_rewards.append(state['reward'])\n",
    "                \n",
    "                if profiler:\n",
    "                    profiler.stop(\"environment step\")\n",
    "\n",
    "            # Track fire rate and terminal rewards for non-fired episodes\n",
    "            epoch_batch_rewards.extend(episode_rewards)\n",
    "\n",
    "            if profiler:\n",
    "                profiler.start(\"calculate causal returns\")\n",
    "\n",
    "            # Calcualte causal reward\n",
    "            causal_returns = []\n",
    "            rolling_causal_return = np.zeros(len(episode_rewards[0]))\n",
    "            # Calculate causal returns in reverse order\n",
    "            for rewards in episode_rewards[::-1]:\n",
    "                causal_return = rewards + training_config.discount_factor * rolling_causal_return\n",
    "                rolling_causal_return = causal_return\n",
    "                causal_returns.append(causal_return)\n",
    "            # Reverse the causal returns to get the correct order\n",
    "            causal_returns = causal_returns[::-1]\n",
    "            \n",
    "            if profiler:\n",
    "                profiler.stop(\"calculate causal returns\")\n",
    "                profiler.stop(\"episode\")\n",
    "\n",
    "            for state, action, reward, causal_return in zip(episode_states, episode_actions, episode_rewards, causal_returns):\n",
    "                batch.append({'s_t': state, 'a_t': action, 'r_t': reward, 'cr_t': causal_return})\n",
    "\n",
    "        # Calculate epoch performance\n",
    "        average_epoch_rewards.append(np.mean(epoch_batch_rewards))\n",
    "        \n",
    "        # Checkpoint if applicable\n",
    "        if epoch % training_config.checkpoint_freq == 0:\n",
    "            agent.checkpoint(model_name, starting_epoch + num_epochs)\n",
    "            with open(f'checkpoints/{model_name}/epoch_rewards.csv', 'w', newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(epoch_batch_rewards)\n",
    "\n",
    "        log_probs = []\n",
    "        batch_causal_returns = []\n",
    "        for step in batch:    # Loop over batch\n",
    "            states = step['s_t']\n",
    "            actions = step['a_t']\n",
    "            if profiler:\n",
    "                profiler.start(\"action prob generation\")\n",
    "            action_probs = agent.generate_action_probs(actions, states)    # Compute action probability from policy\n",
    "            if profiler:\n",
    "                profiler.stop(\"action prob generation\")\n",
    "            log_probs.append(action_probs.squeeze())    # Record the log probability of the chosen action\n",
    "            batch_causal_returns.append(step['cr_t'])\n",
    "\n",
    "        if profiler:\n",
    "            profiler.start(\"update gradient\")\n",
    "\n",
    "        # Perform baselining\n",
    "        baseline = np.mean(batch_causal_returns)\n",
    "        baselined_causal_returns = batch_causal_returns - baseline\n",
    "\n",
    "        baselined_causal_returns = torch.tensor(baselined_causal_returns)\n",
    "        log_probs = torch.stack(log_probs)    #reshape to compute gradient over the whole episode #Shape: (batch_size, 4)\n",
    "\n",
    "        objective = -torch.sum(log_probs * baselined_causal_returns)/len(batch) # Batch, and do \"-\" to convert \"loss\" to \"gain\"\n",
    "\n",
    "        agent.optimizer.zero_grad()    #zero gradients from the previous step\n",
    "        objective.backward()    #compute gradients\n",
    "        agent.optimizer.step()\n",
    "        \n",
    "        if profiler:\n",
    "            profiler.stop(\"update gradient\")\n",
    "            profiler.stop(\"epoch\")\n",
    "\n",
    "    agent.checkpoint(model_name, starting_epoch + num_epochs)\n",
    "    with open(f'checkpoints/{model_name}/epoch_rewards.csv', 'w', newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(epoch_batch_rewards)\n",
    "\n",
    "    return epoch_batch_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 10/10 [00:08<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "agent_config = AgentConfig()\n",
    "training_config = TrainingConfig()\n",
    "agent = Agent(agent_config)\n",
    "env = TheGang()\n",
    "profiler = Profiler()\n",
    "epoch_rewards = train_agent(agent, env, training_config, 10, 0, 'Profiling', profiler=profiler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action generation    | 0.0005 s Â± 0.0011 s per iteration | n_iters = 4000\n",
      "environment step     | 0.0004 s Â± 0.0011 s per iteration | n_iters = 4000\n",
      "calculate causal ret | 0.0000 s Â± 0.0000 s per iteration | n_iters = 1000\n",
      "episode              | 0.0040 s Â± 0.0065 s per iteration | n_iters = 1000\n",
      "action prob generati | 0.0004 s Â± 0.0734 s per iteration | n_iters = 4000\n",
      "update gradient      | 0.2393 s Â± 0.0219 s per iteration | n_iters = 10\n",
      "epoch                | 0.8035 s Â± 0.0725 s per iteration | n_iters = 10\n"
     ]
    }
   ],
   "source": [
    "profiler.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 358/40000 [07:12<13:18:26,  1.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_946111/828237330.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTheGang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfire_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_terminal_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Baseline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_946111/3663019612.py\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(agent, env, training_config, num_epochs, starting_epoch, model_name)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mepisode_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/The-Gang-Bot/src/game.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mcard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhand\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhands\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mhand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# Increment the turn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/The-Gang-Bot/src/hand.py\u001b[0m in \u001b[0;36madd_card\u001b[0;34m(self, card)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecalculate_hand_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/The-Gang-Bot/src/hand.py\u001b[0m in \u001b[0;36mrecalculate_hand_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_house_potential\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_house_potential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfour_of_a_kind_potential\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquad_potential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstraight_flush_potential\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstraight_flush_potential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGH_CARD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# TODO - matching error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\"{self.__class__.__name__}\" object has no field \"{name}\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extra'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'allow'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_extra\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_extra\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_extra__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent_config = AgentConfig()\n",
    "training_config = TrainingConfig()\n",
    "agent = Agent(agent_config)\n",
    "env = TheGang()\n",
    "epoch_rewards = train_agent(agent, env, training_config, 40000, 0, 'Baseline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
